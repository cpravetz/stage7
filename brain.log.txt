2025-06-26 16:33:11.146 | RSA private key for plugin signing not found (this is normal for most services)
2025-06-26 16:33:11.177 | Loaded RSA public key for plugin verification
2025-06-26 16:33:11.600 | Attempting to connect to RabbitMQ (attempt 1/20)...
2025-06-26 16:33:11.600 | Using RabbitMQ URL: amqp://stage7:stage7password@rabbitmq:5672
2025-06-26 16:33:11.600 | Attempting to connect to RabbitMQ host: rabbitmq
2025-06-26 16:33:11.600 | Connecting to RabbitMQ at amqp://stage7:stage7password@rabbitmq:5672
2025-06-26 16:33:11.626 | Attempting to register with Consul (attempt 1/10)...
2025-06-26 16:33:11.626 | Using Consul URL: consul:8500
2025-06-26 16:33:11.844 | [PerformanceTracker] Initialized with empty performance data
2025-06-26 16:33:11.844 | Checking for excessive blacklists...
2025-06-26 16:33:11.867 | No excessive blacklists found
2025-06-26 16:33:11.943 | Brain service listening at http://0.0.0.0:5070
2025-06-26 16:33:11.951 | [shouldBypassAuth] Bypassing auth for auth path: /v1/agent/service/register (matched /register)
2025-06-26 16:33:12.084 | Anthropic Service created, ApiKey starts sk-ant
2025-06-26 16:33:12.085 | Loaded service: AntService
2025-06-26 16:33:12.101 | GG Gemini Service created, ApiKey starts AIzaSy
2025-06-26 16:33:12.101 | Loaded service: GGService
2025-06-26 16:33:12.245 | Gemini Service created, ApiKey starts AIzaSy
2025-06-26 16:33:12.246 | Loaded service: gemini
2025-06-26 16:33:12.259 | Groq Service created, ApiKey starts gsk_m0
2025-06-26 16:33:12.260 | GroqService initialized with API key: Set (length: 56)
2025-06-26 16:33:12.261 | Loaded service: groq
2025-06-26 16:33:12.274 | Huggingface Service created with API key: Set (length: 37)
2025-06-26 16:33:12.275 | Loaded service: HFService
2025-06-26 16:33:12.302 | Mistral Service created, ApiKey starts AhDwC8
2025-06-26 16:33:12.303 | Loaded service: MistralService
2025-06-26 16:33:12.317 | OpenAI Service created, ApiKey starts sk-LaE
2025-06-26 16:33:12.317 | Loaded service: OAService
2025-06-26 16:33:12.319 | OpenRouter Service created, ApiKey starts sk-or-
2025-06-26 16:33:12.319 | Loaded service: ORService
2025-06-26 16:33:12.331 | Openweb Service created, ApiKey starts eyJhbG
2025-06-26 16:33:12.331 | Using default OpenWebUI URL: https://knllm.dusdusdusd.com
2025-06-26 16:33:12.332 | Loaded service: OWService
2025-06-26 16:33:12.332 | modelManager Loaded 9 services.
2025-06-26 16:33:12.377 | Loaded interface: anthropic
2025-06-26 16:33:12.377 | Loaded interface: gemini
2025-06-26 16:33:13.964 | Loaded interface: groq
2025-06-26 16:33:13.983 | [PerformanceTracker] Initialized with empty performance data
2025-06-26 16:33:13.983 | Checking for excessive blacklists...
2025-06-26 16:33:13.983 | No excessive blacklists found
2025-06-26 16:33:13.983 | Loaded interface: huggingface
2025-06-26 16:33:14.014 | Loaded interface: mistral
2025-06-26 16:33:14.016 | Loaded interface: openai
2025-06-26 16:33:14.046 | Loaded interface: openrouter
2025-06-26 16:33:14.060 | OpenWebUIInterface initialized with DEFAULT_TIMEOUT: 300000ms
2025-06-26 16:33:14.060 | Loaded interface: openwebui
2025-06-26 16:33:14.061 | modelManager Loaded 8 interfaces.
2025-06-26 16:33:14.199 | Loaded model: hf/meta-llama/llama-3.2-3b-instruct
2025-06-26 16:33:14.213 | Loaded model: suno/bark
2025-06-26 16:33:14.213 | Loaded model: anthropic/claude-3-haiku-20240307
2025-06-26 16:33:14.213 | Loaded model: anthropic/claude-3-haiku-20240307
2025-06-26 16:33:14.214 | Loaded model: anthropic/claude-2
2025-06-26 16:33:14.230 | Loaded model: codellama/CodeLlama-34b-Instruct-hf
2025-06-26 16:33:14.234 | Loaded model: THUDM/cogvlm-chat-hf
2025-06-26 16:33:14.255 | Loaded model: openai/dall-e-2
2025-06-26 16:33:14.259 | Loaded model: openai/dall-e-3
2025-06-26 16:33:14.261 | Loaded model: deepseek-ai/DeepSeek-R1
2025-06-26 16:33:14.263 | Loaded model: openai/whisper-large-v3
2025-06-26 16:33:14.265 | Loaded model: google/gemini-1.5-pro-vision
2025-06-26 16:33:14.266 | Loaded model: openai/gpt-4.1-nano
2025-06-26 16:33:14.268 | Loaded model: openai/gpt-4-vision-preview
2025-06-26 16:33:14.269 | Loaded model: nousresearch/hermes-3-llama-3.1-405b
2025-06-26 16:33:14.278 | KNLLMModel initialized with OpenWebUI interface
2025-06-26 16:33:14.279 | Loaded model: openweb/knownow
2025-06-26 16:33:14.280 | Loaded model: liquid/lfm-40b
2025-06-26 16:33:14.282 | Loaded model: meta-llama/llama-3.2-11b-vision-instruct
2025-06-26 16:33:14.283 | GroqService availability check: Available
2025-06-26 16:33:14.284 | GroqService API key: Set (length: 56)
2025-06-26 16:33:14.284 | GroqService API URL: https://api.groq.com/openai/v1
2025-06-26 16:33:14.284 | GroqService ready state: Ready
2025-06-26 16:33:14.284 | GroqService is available and ready to use.
2025-06-26 16:33:14.284 | Loaded model: groq/llama-4
2025-06-26 16:33:14.297 | Loaded model: meta-llama/Llama-2-70b-chat-hf
2025-06-26 16:33:14.297 | Loaded model: liuhaotian/llava-v1.5-13b
2025-06-26 16:33:14.300 | Loaded model: microsoft/Phi-3.5-vision-instruct
2025-06-26 16:33:14.303 | MistralService availability check: Available
2025-06-26 16:33:14.306 | MistralService API key: Set
2025-06-26 16:33:14.306 | MistralService API URL: https://api.mistral.ai/v1
2025-06-26 16:33:14.306 | MistralService is available and ready to use.
2025-06-26 16:33:14.306 | Loaded model: mistral/mistral-small-latest
2025-06-26 16:33:14.308 | Loaded model: mistralai/Mistral-Nemo-Instruct-2407
2025-06-26 16:33:14.317 | Loaded model: facebook/musicgen-large
2025-06-26 16:33:14.319 | MistralService availability check: Available
2025-06-26 16:33:14.319 | MistralService API key: Set
2025-06-26 16:33:14.343 | MistralService API URL: https://api.mistral.ai/v1
2025-06-26 16:33:14.343 | MistralService is available and ready to use.
2025-06-26 16:33:14.372 | Loaded model: mistral/pixtral-12B-2409
2025-06-26 16:33:14.372 | GroqService availability check: Available
2025-06-26 16:33:14.372 | GroqService API key: Set (length: 56)
2025-06-26 16:33:14.372 | GroqService API URL: https://api.groq.com/openai/v1
2025-06-26 16:33:14.372 | GroqService ready state: Ready
2025-06-26 16:33:14.372 | GroqService is available and ready to use.
2025-06-26 16:33:14.372 | Loaded model: groq/qwen-qwq-32b
2025-06-26 16:33:14.372 | Loaded model: facebook/seamless-m4t-large
2025-06-26 16:33:14.372 | Loaded model: stabilityai/stable-diffusion-xl-base-1.0
2025-06-26 16:33:14.372 | Loaded model: bigcode/starcoder
2025-06-26 16:33:14.372 | Loaded model: openai/tts
2025-06-26 16:33:14.398 | Loaded model: openai/whisper-large-v3
2025-06-26 16:33:14.399 | Loaded model: openai/whisper
2025-06-26 16:33:14.399 | modelManager Loaded 31 models.
2025-06-26 16:33:14.446 | [shouldBypassAuth] Bypassing auth for auth path: /registerComponent (matched /register)
2025-06-26 16:33:14.513 | Using default Librarian URL: librarian:5040
2025-06-26 16:33:14.541 | Service Brain registered with Consul
2025-06-26 16:33:14.545 | Successfully registered Brain with Consul
2025-06-26 16:33:14.596 | Brain registered successfully with PostOffice
2025-06-26 16:33:16.915 | Created ServiceTokenManager for Brain
2025-06-26 16:33:21.891 | [Brain] Syncing model performance data to Librarian...
2025-06-26 16:33:21.891 | [PerformanceTracker] Getting all performance data: 0 models
2025-06-26 16:33:21.891 | [Brain] Performance data contains 0 models
2025-06-26 16:33:21.891 | [Brain] No model performance data available to sync, skipping
2025-06-26 16:33:21.893 | [Brain] Current active model requests: 0
2025-06-26 16:33:34.563 | Connected to RabbitMQ
2025-06-26 16:33:34.569 | Channel created successfully
2025-06-26 16:33:34.569 | RabbitMQ channel ready
2025-06-26 16:33:34.650 | Connection test successful - RabbitMQ connection is stable
2025-06-26 16:33:34.650 | Creating queue: brain-Brain
2025-06-26 16:33:34.663 | Binding queue to exchange: stage7
2025-06-26 16:33:34.679 | Successfully connected to RabbitMQ and set up queues/bindings
2025-06-26 16:33:36.309 | [Brain] Skipping authentication for exempt path: /health
2025-06-26 16:34:03.869 | [Brain] Skipping authentication for exempt path: /chat
2025-06-26 16:34:03.870 | Chat request received
2025-06-26 16:34:03.871 | Selecting model for optimization: accuracy, conversationType: text/text
2025-06-26 16:34:03.871 | **** CACHE MISS **** No cached result for key: accuracy-text/text
2025-06-26 16:34:03.871 | Cache miss or expired. Selecting model from scratch.
2025-06-26 16:34:03.871 | Total models loaded: 31
2025-06-26 16:34:03.872 | Evaluating model: hf/meta-llama/llama-3.2-3b-instruct, interface: huggingface, service: HFService
2025-06-26 16:34:03.872 | Model hf/meta-llama/llama-3.2-3b-instruct is available for selection
2025-06-26 16:34:03.872 | Evaluating model: suno/bark, interface: huggingface, service: HFService
2025-06-26 16:34:03.872 | Skipping model suno/bark because it doesn't support conversation type text/text
2025-06-26 16:34:03.872 | Evaluating model: anthropic/claude-3-haiku-20240307, interface: anthropic, service: AntService
2025-06-26 16:34:03.872 | Model anthropic/claude-3-haiku-20240307 is available for selection
2025-06-26 16:34:03.872 | Evaluating model: anthropic/claude-2, interface: openrouter, service: ORService
2025-06-26 16:34:03.872 | Model anthropic/claude-2 is available for selection
2025-06-26 16:34:03.872 | Evaluating model: codellama/CodeLlama-34b-Instruct-hf, interface: huggingface, service: HFService
2025-06-26 16:34:03.872 | Skipping model codellama/CodeLlama-34b-Instruct-hf because it doesn't support conversation type text/text
2025-06-26 16:34:03.872 | Evaluating model: THUDM/cogvlm-chat-hf, interface: huggingface, service: HFService
2025-06-26 16:34:03.872 | Skipping model THUDM/cogvlm-chat-hf because it doesn't support conversation type text/text
2025-06-26 16:34:03.872 | Evaluating model: openai/dall-e-2, interface: openai, service: OAService
2025-06-26 16:34:03.872 | Skipping model openai/dall-e-2 because it doesn't support conversation type text/text
2025-06-26 16:34:03.872 | Evaluating model: openai/dall-e-3, interface: openai, service: OAService
2025-06-26 16:34:03.872 | Skipping model openai/dall-e-3 because it doesn't support conversation type text/text
2025-06-26 16:34:03.872 | Evaluating model: deepseek-ai/DeepSeek-R1, interface: huggingface, service: HFService
2025-06-26 16:34:03.872 | Skipping model deepseek-ai/DeepSeek-R1 because it doesn't support conversation type text/text
2025-06-26 16:34:03.872 | Evaluating model: openai/whisper-large-v3, interface: huggingface, service: HFService
2025-06-26 16:34:03.872 | Skipping model openai/whisper-large-v3 because it doesn't support conversation type text/text
2025-06-26 16:34:03.872 | Evaluating model: google/gemini-1.5-pro-vision, interface: gemini, service: gemini
2025-06-26 16:34:03.873 | Model google/gemini-1.5-pro-vision is available for selection
2025-06-26 16:34:03.873 | Evaluating model: openai/gpt-4.1-nano, interface: openai, service: OAService
2025-06-26 16:34:03.873 | Model openai/gpt-4.1-nano is available for selection
2025-06-26 16:34:03.873 | Evaluating model: openai/gpt-4-vision-preview, interface: openrouter, service: ORService
2025-06-26 16:34:03.873 | Model openai/gpt-4-vision-preview is available for selection
2025-06-26 16:34:03.873 | Evaluating model: nousresearch/hermes-3-llama-3.1-405b, interface: openrouter, service: ORService
2025-06-26 16:34:03.873 | Model nousresearch/hermes-3-llama-3.1-405b is available for selection
2025-06-26 16:34:03.873 | Evaluating model: openweb/knownow, interface: openwebui, service: OWService
2025-06-26 16:34:03.873 | Model openweb/knownow is available for selection
2025-06-26 16:34:03.873 | Evaluating model: liquid/lfm-40b, interface: openrouter, service: ORService
2025-06-26 16:34:03.873 | Model liquid/lfm-40b is available for selection
2025-06-26 16:34:03.873 | Evaluating model: meta-llama/llama-3.2-11b-vision-instruct, interface: openrouter, service: ORService
2025-06-26 16:34:03.873 | Skipping model meta-llama/llama-3.2-11b-vision-instruct because it doesn't support conversation type text/text
2025-06-26 16:34:03.873 | Evaluating model: groq/llama-4, interface: groq, service: groq
2025-06-26 16:34:03.873 | GroqService availability check: Available
2025-06-26 16:34:03.873 | GroqService API key: Set (length: 56)
2025-06-26 16:34:03.874 | GroqService API URL: https://api.groq.com/openai/v1
2025-06-26 16:34:03.874 | GroqService ready state: Ready
2025-06-26 16:34:03.874 | GroqService is available and ready to use.
2025-06-26 16:34:03.874 | Model groq/llama-4 is available for selection
2025-06-26 16:34:03.874 | Evaluating model: meta-llama/Llama-2-70b-chat-hf, interface: huggingface, service: HFService
2025-06-26 16:34:03.874 | Model meta-llama/Llama-2-70b-chat-hf is available for selection
2025-06-26 16:34:03.874 | Evaluating model: liuhaotian/llava-v1.5-13b, interface: huggingface, service: HFService
2025-06-26 16:34:03.874 | Skipping model liuhaotian/llava-v1.5-13b because it doesn't support conversation type text/text
2025-06-26 16:34:03.874 | Evaluating model: microsoft/Phi-3.5-vision-instruct, interface: huggingface, service: HFService
2025-06-26 16:34:03.874 | Skipping model microsoft/Phi-3.5-vision-instruct because it doesn't support conversation type text/text
2025-06-26 16:34:03.874 | Evaluating model: mistral/mistral-small-latest, interface: mistral, service: MistralService
2025-06-26 16:34:03.874 | MistralService availability check: Available
2025-06-26 16:34:03.874 | MistralService API key: Set
2025-06-26 16:34:03.874 | MistralService API URL: https://api.mistral.ai/v1
2025-06-26 16:34:03.874 | MistralService is available and ready to use.
2025-06-26 16:34:03.874 | Model mistral/mistral-small-latest is available for selection
2025-06-26 16:34:03.874 | Evaluating model: mistralai/Mistral-Nemo-Instruct-2407, interface: huggingface, service: HFService
2025-06-26 16:34:03.874 | Model mistralai/Mistral-Nemo-Instruct-2407 is available for selection
2025-06-26 16:34:03.874 | Evaluating model: facebook/musicgen-large, interface: huggingface, service: HFService
2025-06-26 16:34:03.874 | Skipping model facebook/musicgen-large because it doesn't support conversation type text/text
2025-06-26 16:34:03.874 | Evaluating model: mistral/pixtral-12B-2409, interface: mistral, service: MistralService
2025-06-26 16:34:03.874 | MistralService availability check: Available
2025-06-26 16:34:03.874 | MistralService API key: Set
2025-06-26 16:34:03.874 | MistralService API URL: https://api.mistral.ai/v1
2025-06-26 16:34:03.874 | MistralService is available and ready to use.
2025-06-26 16:34:03.874 | Model mistral/pixtral-12B-2409 is available for selection
2025-06-26 16:34:03.874 | Evaluating model: groq/qwen-qwq-32b, interface: groq, service: groq
2025-06-26 16:34:03.874 | GroqService availability check: Available
2025-06-26 16:34:03.874 | GroqService API key: Set (length: 56)
2025-06-26 16:34:03.874 | GroqService API URL: https://api.groq.com/openai/v1
2025-06-26 16:34:03.874 | GroqService ready state: Ready
2025-06-26 16:34:03.875 | GroqService is available and ready to use.
2025-06-26 16:34:03.875 | Model groq/qwen-qwq-32b is available for selection
2025-06-26 16:34:03.875 | Evaluating model: facebook/seamless-m4t-large, interface: huggingface, service: HFService
2025-06-26 16:34:03.875 | Skipping model facebook/seamless-m4t-large because it doesn't support conversation type text/text
2025-06-26 16:34:03.875 | Evaluating model: stabilityai/stable-diffusion-xl-base-1.0, interface: huggingface, service: HFService
2025-06-26 16:34:03.875 | Skipping model stabilityai/stable-diffusion-xl-base-1.0 because it doesn't support conversation type text/text
2025-06-26 16:34:03.875 | Evaluating model: bigcode/starcoder, interface: huggingface, service: HFService
2025-06-26 16:34:03.875 | Skipping model bigcode/starcoder because it doesn't support conversation type text/text
2025-06-26 16:34:03.875 | Evaluating model: openai/tts, interface: openai, service: OAService
2025-06-26 16:34:03.875 | Skipping model openai/tts because it doesn't support conversation type text/text
2025-06-26 16:34:03.875 | Evaluating model: openai/whisper, interface: openai, service: OAService
2025-06-26 16:34:03.875 | Skipping model openai/whisper because it doesn't support conversation type text/text
2025-06-26 16:34:03.876 | Using score-based model selection. Top model: hf/meta-llama/llama-3.2-3b-instruct
2025-06-26 16:34:03.876 | Selected model hf/meta-llama/llama-3.2-3b-instruct for accuracy optimization and conversation type text/text
2025-06-26 16:34:03.876 | Chatting with model meta-llama/llama-3.2-3b-instruct using interface huggingface and conversation type text/text
2025-06-26 16:34:03.876 | Chat messages provided: [
2025-06-26 16:34:03.876 |   {
2025-06-26 16:34:03.876 |     "role": "user",
2025-06-26 16:34:03.876 |     "content": "You are a planning assistant. Your ONLY task is to generate one of the following JSON outputs to achieve the following goal: 'Handle the action verb \"ACCOMPLISH\" in our plan with the following context:  Initial mission step with inputs [object Object] by defining a plan, generating an answer from the inputs, or recommending a new plugin for handling the actionVerb. Respond with a plan, a plugin request, or a literal result. Avoid using this action verb, ACCOMPLISH, in the plan.'.\n\nYou MUST respond with ONLY a JSON object in ONLY ONE of these three formats. DO NOT include any explanations, markdown formatting, or additional text outside the JSON object.\n\n\n1. If the goal can be sub-divided into smaller steps, respond with a plan as a JSON object in this format:\n\n{\n    \"type\": \"PLAN\",\n    \"context\": \"Any overarching points or introduction to the plan you want to share\",\n    \"plan\": [\n        {\n            \"number\": 1,\n            \"actionVerb\": \"DESCRIPTIVE_ACTION_VERB\",\n            \"description\": \"Brief description of the step\",\n            \"inputs\": {\n                \"inputName1\": {\"value\": \"predeterminedValue\"},\n                \"inputName2\": {\"outputKey\": \"outputKeyFromPreviousStep\"}\n            },\n            \"dependencies\": {},\n            \"outputs\": {\n                \"outputKey1\": \"Description of output1\",\n                \"outputKey2\": \"Description of output2\"\n            },\n            \"recommendedRole\": \"coordinator\"\n        },\n        {\n            \"number\": 2,\n            \"actionVerb\": \"ANOTHER_ACTION\",\n            \"description\": \"Description of another step\",\n            \"inputs\": {\n                \"inputName3\": {\"outputKey\": \"outputKey2\"}\n            },\n            \"dependencies\": {\"outputKey2\": 1},\n            \"outputs\": {\n                \"outputKey3\": \"Description of output3\"\n            },\n            \"recommendedRole\": \"researcher\"\n        }\n    ]\n}\n\nGuidelines for creating a plan:\n1. Number each step sequentially using the \"number\" field.\n2. Use specific, actionable verbs or phrases for each step using the \"actionVerb\" field (e.g., ANALYZE_CSV, ANALYZE_AUDIOFILE, PREDICT, WRITE_TEXT, WRITE_CODE, BOOK_A_CAR).\n3. The schema of each step MUST be exactly as defined above. Every field is mandatory, but the \"inputs\" field may be an empty object ({}).\n4. Each step input in the \"inputs\" object MUST be an object with EITHER a 'value' property for predetermined values OR an 'outputKey' property referencing an output from a previous step. DO NOT include \"inputName\", \"inputValue\", or \"args\" within the input definition objects inside the \"inputs\" field.\n5. List dependencies for each step as an object in the \"dependencies\" field, where property names are the output keys needed and values are the step numbers that provide the required output (e.g., {\"outputname\": 1}). There MUST be a dependency entry for every input that comes from a previous step output.\n6. Specify the outputs of each step in the \"outputs\" field. At least one output is mandatory.\n7. Aim for 5-10 steps in the plan, breaking down complex tasks if necessary.\n8. Be thorough in your \"description\" fields. This is the only instruction the performer will have.\n9. Ensure the final step produces the desired outcome or mission of the goal.\n10. The actionVerb DELEGATE is available to use to create sub-agents with goals of their own.\n11. Input values may be determined by preceding steps. In those instances, use the 'outputKey' reference. For inputs whose values are not yet determined but will be provided at runtime (e.g., from user input), set the 'value' to `null`.\n12. For each step, include a \"recommendedRole\" field with one of the available agent roles that would be best suited for the task.\n13. Every step must have at least one output.\n\nAvailable Agent Roles:\n- coordinator: Coordinates activities of other agents, manages task allocation, and ensures mission success. Good for planning, delegation, and monitoring.\n- researcher: Gathers, analyzes, and synthesizes information from various sources. Good for information gathering and data analysis.\n- creative: Generates creative ideas, content, and solutions to problems. Good for idea generation and content creation.\n- critic: Evaluates ideas, plans, and content, providing constructive feedback. Good for quality assessment and risk identification.\n- executor: Implements plans and executes tasks with precision and reliability. Good for task execution and process following.\n- domain_expert: Provides specialized knowledge and expertise in a specific domain. Good for technical analysis and expert advice.\n\nIMPORTANT: Your response MUST be a valid JSON object with no additional text or formatting. The JSON must start with { and end with } and must include one of the three types: \"DIRECT_ANSWER\", \"PLAN\", or \"PLUGIN\".\n\nPlugins are available to execute steps of the plan. Some have required inputs - required properties for the inputs object. These plugins include:\n- ACCOMPLISH: Accomplishes a given goal or creates a plan to achieve it\n    Required Inputs:\n      - goal (string) [required]: The goal to be accomplished or planned for\n- FILE_OPERATION: Provides services for file operations: read, write, append\n    Required Inputs:\n      - path (string) [required]: The path for the filename to read, write, or append content (relative paths only for security)\n      - operation (string) [required]: Operation to perform: 'read', 'write', or 'append'\n      - content (string): For write and append operations, the content to write or append\n- GET_USER_INPUT: Requests input from the user\n    Required Inputs:\n      - question (string) [required]: The question to ask the user\n      - choices (array): Optional array of choices for multiple choice questions\n      - answerType (string): Type of answer expected (text, number, boolean, or multipleChoice)\n- SCRAPE: Scrapes content from a given URL\n    Required Inputs:\n      - url (string) [required]: The URL to scrape content from\n      - selector (string): CSS selector to target specific elements (optional)\n      - attribute (string): Attribute to extract from the selected elements (optional)\n      - limit (number): Maximum number of results to return (optional)\n- SEARCH: Searches DuckDuckGo for a given term and returns a list of links\n    Required Inputs:\n      - searchTerm (string) [required]: The term to search for on DuckDuckGo\n- DELEGATE: Create sub-agents with goals of their own.\n- ACCOMPLISH - takes a specific goal and either achieves it or returns a plan to achieve it. (required input: goal)\n- THINK - sends prompts to the chat function of the LLMs attached to the system in order to generate content from a conversation.(required input: prompt) (optional inputs: optimization (cost|accuracy|creativity|speed|continuity), ConversationType) accuracy is the default optimization\n- GENERATE - uses LLM services to generate content from a prompt or other content. Services include image creation, audio transscription, image editing, etc. (required input: ConversationType) (optional inputs: modelName, optimization, prompt, file, audio, video, image...)\n- FILE_OPS - provides services for file operations read, write, append (required inputs: path, operation, content)\n- SEARCH - searches DuckDuckGo for a given term and returns a list of links (required input: searchTerm)\n- SCRAPE - scrapes content from a given URL (required inputs: url, selector, attribute, limit)\n- GET_USER_INPUT - requests input from the user (required inputs: question, answerType) (optional input: choices)\n- DECIDE - Conditional branching based on a condition (required inputs: condition: {\"inputName\": \"value\"}, trueSteps[], falseSteps[])\n- WHILE - Repeat steps while a condition is true (required inputs: condition: {\"inputName\": \"value\"}, steps[])\n- UNTIL - Repeat steps until a condition becomes true (required inputs: condition: {\"inputName\": \"value\"}, steps[])\n- SEQUENCE - Execute steps in strict sequential order / no concurrency (required inputs: steps[])\n- TIMEOUT - Set a timeout for a group of steps (required inputs: timeout, steps[])\n- REPEAT - Repeat steps a specific number of times (required inputs: count, steps[])\n\n2. When the goal is discrete and can be accomplished most efficiently with a new plugin, define one. Creating a plugin should be avoided when the goal can be accomplished with a plan. If you determine a plugin is needed, respond with a JSON object in this format:\n\n{\n    \"type\": \"PLUGIN\",\n    \"plugin\": {\n        \"id\": \"plugin-{verb}\",\n        \"verb\": \"{verb}\",\n        \"description\": \"A short description of the plugin\",\n        \"explanation\": \"A more complete description including inputs, process overview, and outputs than a software engineer can use to build the plugin\",\n        \"inputDefinitions\": [\n            {\n                \"name\": \"{input name}\",\n                \"required\": true/false,\n                \"type\": \"string\",\n                \"description\": \"Brief explanation of the input\"\n            },\n            // ... more inputs ...\n        ]\n    }\n}\n\n3. If you have the ability to provide a full and complete response that resolves the goal, respond with a JSON object in this format:\n\n{\n    \"type\": \"DIRECT_ANSWER\",\n    \"answer\": \"Your direct answer here\"\n}\n\nFor example, if the goal is \"add two and three\", you can respond with {\"type\": \"DIRECT_ANSWER\", \"answer\": 5}, or if the goal is \"Write a memo\", you can write the memo and return it as the answer value.\n\nPlans, direct_answers and plugins are mutually exclusive. Do not return plans or plugins as direct_answers. You can only respond with one of the three types.\n\nMission Context: No overall mission context provided."
2025-06-26 16:34:03.876 |   }
2025-06-26 16:34:03.876 | ]
2025-06-26 16:34:03.876 | First message content length: 9654 characters
2025-06-26 16:34:03.877 | [ModelManager] Tracking model request: 34894626-46a6-4956-92b9-88e7a9e12004 for model meta-llama/llama-3.2-3b-instruct, conversation type text/text
2025-06-26 16:34:03.877 | [PerformanceTracker] Tracking request 34894626-46a6-4956-92b9-88e7a9e12004 for model meta-llama/llama-3.2-3b-instruct, conversation type text/text
2025-06-26 16:34:03.877 | [PerformanceTracker] Request history size: 1
2025-06-26 16:34:03.877 | [PerformanceTracker] Current performance data contains 0 models
2025-06-26 16:34:03.877 | [ModelManager] Active requests count: 1
2025-06-26 16:34:03.877 | Brain: Passing optionals to model: {"modelName":"meta-llama/llama-3.2-3b-instruct"}
2025-06-26 16:34:03.878 | Token allocation: input=2610, max_new=1286, total=3896
2025-06-26 16:34:04.271 | Error in Huggingface stream: Server response contains error: 404
2025-06-26 16:34:04.455 | Model response received: Error: An error occurred while fetching the blob
2025-06-26 16:34:04.455 | Error generating response from Huggingface: An error occurred while fetching the blob
2025-06-26 16:34:04.457 | Original response length: 48
2025-06-26 16:34:04.457 | First 200 chars: Error: An error occurred while fetching the blob
2025-06-26 16:34:04.458 | Response is not valid JSON after initial cleaning, attempting extraction/repair.
2025-06-26 16:34:04.458 | Could not extract valid JSON from response using patterns. Trying common repair strategies.
2025-06-26 16:34:04.458 | Could not extract or repair valid JSON from response Error: An error occurred while fetching the blob
2025-06-26 16:34:04.458 | [Brain] Estimated token count for response: 12
2025-06-26 16:34:04.459 | [ModelManager] Tracking model response for request 34894626-46a6-4956-92b9-88e7a9e12004, success: true, token count: 12
2025-06-26 16:34:04.459 | [ModelManager] Found active request for model meta-llama/llama-3.2-3b-instruct, conversation type text/text
2025-06-26 16:34:04.459 | [PerformanceTracker] Tracking response for request 34894626-46a6-4956-92b9-88e7a9e12004, success: true, token count: 12
2025-06-26 16:34:04.459 | [PerformanceTracker] Found request data for model meta-llama/llama-3.2-3b-instruct, conversation type text/text
2025-06-26 16:34:04.459 | [PerformanceTracker] Request latency: 582ms
2025-06-26 16:34:04.460 | [PerformanceTracker] Updating metrics for model meta-llama/llama-3.2-3b-instruct, conversation type text/text, success: true
2025-06-26 16:34:04.460 | [PerformanceTracker] Creating new model data for meta-llama/llama-3.2-3b-instruct
2025-06-26 16:34:04.460 | [PerformanceTracker] Creating new metrics for meta-llama/llama-3.2-3b-instruct, conversation type text/text
2025-06-26 16:34:04.460 | [PerformanceTracker] Incremented usage count for meta-llama/llama-3.2-3b-instruct from 0 to 1
2025-06-26 16:34:04.460 | [PerformanceTracker] Incremented success count for meta-llama/llama-3.2-3b-instruct from 0 to 1
2025-06-26 16:34:04.460 | [PerformanceTracker] Updated success rate for meta-llama/llama-3.2-3b-instruct from 0.00 to 1.00
2025-06-26 16:34:04.460 | [PerformanceTracker] Updated average latency for meta-llama/llama-3.2-3b-instruct from 0.00ms to 582.00ms
2025-06-26 16:34:04.460 | [PerformanceTracker] Updated average token count for meta-llama/llama-3.2-3b-instruct from 0.00 to 12.00
2025-06-26 16:34:04.461 | [PerformanceTracker] Updated metrics for model meta-llama/llama-3.2-3b-instruct:
2025-06-26 16:34:04.461 |         - Usage count: 1
2025-06-26 16:34:04.461 |         - Success rate: 1.00
2025-06-26 16:34:04.461 |         - Average latency: 582.00ms
2025-06-26 16:34:04.461 |         - Average token count: 12.00
2025-06-26 16:34:04.461 |         - Consecutive failures: 0
2025-06-26 16:34:04.461 |         - Blacklisted: No
2025-06-26 16:34:04.461 | [PerformanceTracker] Current performance data size: 1 models
2025-06-26 16:34:04.461 | [ModelManager] Successfully tracked response for model meta-llama/llama-3.2-3b-instruct
2025-06-26 16:34:04.461 | [ModelManager] Removed request 34894626-46a6-4956-92b9-88e7a9e12004 from active requests. Remaining: 0
2025-06-26 16:34:04.461 | Response preview (first 100 chars): Error: An error occurred while fetching the blob
2025-06-26 16:34:04.489 | [Brain] Skipping authentication for exempt path: /chat
2025-06-26 16:34:04.489 | Chat request received
2025-06-26 16:34:04.489 | Selecting model for optimization: accuracy, conversationType: text/text
2025-06-26 16:34:04.489 | **** CACHE HIT **** Using cached model selection result: hf/meta-llama/llama-3.2-3b-instruct
2025-06-26 16:34:04.489 | Cache age: 0 seconds
2025-06-26 16:34:04.489 | Chatting with model meta-llama/llama-3.2-3b-instruct using interface huggingface and conversation type text/text
2025-06-26 16:34:04.490 | Chat messages provided: [
2025-06-26 16:34:04.490 |   {
2025-06-26 16:34:04.490 |     "role": "user",
2025-06-26 16:34:04.490 |     "content": "Evaluate the following error report and the associated source code. Provide remediation recommendations including proposed code improvements. Format your response as follows:\n\n        ANALYSIS:\n        [Your analysis here]\n\n        RECOMMENDATIONS:\n        [Your recommendations here]\n\n        CODE SUGGESTIONS:\n        [Your code suggestions here]\n\n        \"end of response\"\n\n        The error is:\n         { \"name\": \"Error\", \"message\": \"An error occurred while fetching the blob\", \"stack\": \"Error: An error occurred while fetching the blob\\n at request (/usr/src/app/node_modules/@huggingface/inference/dist/index.cjs:260:11)\\n at process.processTicksAndRejections (node:internal/process/task_queues:95:5)\\n at async textGeneration (/usr/src/app/node_modules/@huggingface/inference/dist/index.cjs:754:5)\\n at async HuggingfaceInterface.chat (/usr/src/app/services/brain/dist/interfaces/HuggingfaceInterface.js:195:38)\\n at async Brain.chat (/usr/src/app/services/brain/dist/Brain.js:185:41)\\n at async /usr/src/app/services/brain/dist/Brain.js:54:17\" }\n\n        and the source code is:\n         File: /usr/src/app/services/brain/dist/interfaces/HuggingfaceInterface.js\nLine: 195\nColumn: 38\n\n  190:                     this.blacklistAllHuggingfaceModelsUntilNextMonth(streamErrorMessage);\n  191:                     throw new Error(`Huggingface monthly credits exceeded. All Huggingface models have been blacklisted until the first of next month. Error: ${streamErrorMessage}`);\n  192:                 }\n  193:                 // Try a non-streaming fallback\n  194:                 try {\n> 195:                     const response = await inference.textGeneration({ <-- ERROR\n  196:                         model: options.modelName || \"HuggingFaceH4/zephyr-7b-beta\",\n  197:                         inputs: trimmedMessages[trimmedMessages.length - 1].content,\n  198:                         parameters: {\n  199:                             max_new_tokens: max_new_tokens,\n  200:                             temperature: options.temperature || 0.5,\n\n\nFile: /usr/src/app/services/brain/dist/Brain.js\nLine: 185\nColumn: 41\n\n  180:                 // Track the request\n  181:                 const requestId = this.modelManager.trackModelRequest(selectedModel.modelName, thread.conversationType, JSON.stringify(messages));\n  182:                 try {\n  183:                     // Pass optionals to the model, including response_format if specified\n  184:                     console.log(`Brain: Passing optionals to model: ${JSON.stringify(thread.optionals)}`);\n> 185:                     let modelResponse = await selectedModel.chat(messages, thread.optionals || {}); <-- ERROR\n  186:                     console.log(`Model response received:`, modelResponse);\n  187:                     // --- JSON extraction and validation ---\n  188:                     // If the conversation type is text/code or the prompt requests JSON, ensure JSON response\n  189:                     let requireJson = false;\n  190:                     if (thread.conversationType === baseInterface_1.LLMConversationType.TextToCode)\n"
2025-06-26 16:34:04.490 |   }
2025-06-26 16:34:04.490 | ]
2025-06-26 16:34:04.490 | First message content length: 3104 characters
2025-06-26 16:34:04.490 | [ModelManager] Tracking model request: 8ed49e7b-47c8-41cd-8853-8928a133041d for model meta-llama/llama-3.2-3b-instruct, conversation type text/text
2025-06-26 16:34:04.490 | [PerformanceTracker] Tracking request 8ed49e7b-47c8-41cd-8853-8928a133041d for model meta-llama/llama-3.2-3b-instruct, conversation type text/text
2025-06-26 16:34:04.490 | [PerformanceTracker] Request history size: 2
2025-06-26 16:34:04.490 | [PerformanceTracker] Current performance data contains 1 models
2025-06-26 16:34:04.490 | [ModelManager] Active requests count: 1
2025-06-26 16:34:04.490 | Brain: Passing optionals to model: {"modelName":"meta-llama/llama-3.2-3b-instruct"}
2025-06-26 16:34:04.490 | Token allocation: input=839, max_new=3057, total=3896
2025-06-26 16:34:04.579 | Error in Huggingface stream: Server response contains error: 404
2025-06-26 16:34:04.660 | Error analysis already in progress, skipping
2025-06-26 16:34:04.660 | Model response received: Error: An error occurred while fetching the blob
2025-06-26 16:34:04.660 | Original response length: 48
2025-06-26 16:34:04.660 | First 200 chars: Error: An error occurred while fetching the blob
2025-06-26 16:34:04.660 | Response is not valid JSON after initial cleaning, attempting extraction/repair.
2025-06-26 16:34:04.660 | Could not extract valid JSON from response using patterns. Trying common repair strategies.
2025-06-26 16:34:04.660 | Error generating response from Huggingface: An error occurred while fetching the blob
2025-06-26 16:34:04.661 | Could not extract or repair valid JSON from response Error: An error occurred while fetching the blob
2025-06-26 16:34:04.661 | [Brain] Estimated token count for response: 12
2025-06-26 16:34:04.661 | [ModelManager] Tracking model response for request 8ed49e7b-47c8-41cd-8853-8928a133041d, success: true, token count: 12
2025-06-26 16:34:04.661 | [ModelManager] Found active request for model meta-llama/llama-3.2-3b-instruct, conversation type text/text
2025-06-26 16:34:04.661 | [PerformanceTracker] Tracking response for request 8ed49e7b-47c8-41cd-8853-8928a133041d, success: true, token count: 12
2025-06-26 16:34:04.661 | [PerformanceTracker] Found request data for model meta-llama/llama-3.2-3b-instruct, conversation type text/text
2025-06-26 16:34:04.661 | [PerformanceTracker] Request latency: 171ms
2025-06-26 16:34:04.661 | [PerformanceTracker] Updating metrics for model meta-llama/llama-3.2-3b-instruct, conversation type text/text, success: true
2025-06-26 16:34:04.661 | [PerformanceTracker] Incremented usage count for meta-llama/llama-3.2-3b-instruct from 1 to 2
2025-06-26 16:34:04.661 | [PerformanceTracker] Incremented success count for meta-llama/llama-3.2-3b-instruct from 1 to 2
2025-06-26 16:34:04.661 | [PerformanceTracker] Updated success rate for meta-llama/llama-3.2-3b-instruct from 1.00 to 1.00
2025-06-26 16:34:04.661 | [PerformanceTracker] Updated average latency for meta-llama/llama-3.2-3b-instruct from 582.00ms to 540.90ms
2025-06-26 16:34:04.661 | [PerformanceTracker] Updated average token count for meta-llama/llama-3.2-3b-instruct from 12.00 to 12.00
2025-06-26 16:34:04.661 | [PerformanceTracker] Updated metrics for model meta-llama/llama-3.2-3b-instruct:
2025-06-26 16:34:04.661 |         - Usage count: 2
2025-06-26 16:34:04.661 |         - Success rate: 1.00
2025-06-26 16:34:04.661 |         - Average latency: 540.90ms
2025-06-26 16:34:04.661 |         - Average token count: 12.00
2025-06-26 16:34:04.661 |         - Consecutive failures: 0
2025-06-26 16:34:04.661 |         - Blacklisted: No
2025-06-26 16:34:04.662 | [PerformanceTracker] Current performance data size: 1 models
2025-06-26 16:34:04.662 | [ModelManager] Successfully tracked response for model meta-llama/llama-3.2-3b-instruct
2025-06-26 16:34:04.662 | [ModelManager] Removed request 8ed49e7b-47c8-41cd-8853-8928a133041d from active requests. Remaining: 0
2025-06-26 16:34:04.662 | Response preview (first 100 chars): Error: An error occurred while fetching the blob
2025-06-26 16:34:04.665 | 
2025-06-26 16:34:04.665 | 
2025-06-26 16:34:04.665 | **** REMEDIATION GUIDANCE ****
2025-06-26 16:34:04.665 | 
2025-06-26 16:34:04.665 | 
2025-06-26 16:34:04.665 |     Error: An error occurred while fetching the blob
2025-06-26 16:34:04.665 | 
2025-06-26 16:34:04.665 | 
2025-06-26 16:34:04.665 |     Stack: Error: An error occurred while fetching the blob
2025-06-26 16:34:04.665 |     at request (/usr/src/app/node_modules/@huggingface/inference/dist/index.cjs:260:11)
2025-06-26 16:34:04.665 |     at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
2025-06-26 16:34:04.665 |     at async textGeneration (/usr/src/app/node_modules/@huggingface/inference/dist/index.cjs:754:5)
2025-06-26 16:34:04.665 |     at async HuggingfaceInterface.chat (/usr/src/app/services/brain/dist/interfaces/HuggingfaceInterface.js:195:38)
2025-06-26 16:34:04.665 |     at async Brain.chat (/usr/src/app/services/brain/dist/Brain.js:185:41)
2025-06-26 16:34:04.665 |     at async /usr/src/app/services/brain/dist/Brain.js:54:17
2025-06-26 16:34:04.665 | 
2025-06-26 16:34:04.665 | 
2025-06-26 16:34:04.665 |     Remediation Guidance:
2025-06-26 16:34:04.665 | 
2025-06-26 16:34:04.665 |     Error: An error occurred while fetching the blob
2025-06-26 16:34:04.665 | 
2025-06-26 16:34:04.665 | *******************************
2025-06-26 16:34:42.985 | [Brain] Skipping authentication for exempt path: /health
2025-06-26 16:35:25.211 | Checking for excessive blacklists on regular interval...
2025-06-26 16:35:25.211 | Checking for excessive blacklists...
2025-06-26 16:35:25.212 | No excessive blacklists found
2025-06-26 16:35:27.328 | Checking for excessive blacklists on regular interval...
2025-06-26 16:35:27.328 | Checking for excessive blacklists...
2025-06-26 16:35:27.328 | No excessive blacklists found
2025-06-26 16:35:49.663 | [Brain] Skipping authentication for exempt path: /health