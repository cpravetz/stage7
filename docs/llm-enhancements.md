# 1. Comprehensive LLM Documentation

## 1.1. Introduction and Overview

This document consolidates the documentation for LLM (Large Language Model) related functionalities within the Stage7 system. It covers the architecture for plugin-aware LLMs, various enhancements including performance tracking, prompt management, response evaluation, and robust timeout/fallback mechanisms. The goal is to provide a single, comprehensive source of truth for LLM integration and future development.

## 1.2. LLM Architecture

### 1.2.1. Current Challenges

The existing LLM integration faces several challenges that hinder optimal performance and scalability:
*   **Token Limit Overwhelm**: Large lists of available plugins consume excessive LLM input tokens, leading to higher costs and reduced context window for the actual task.
*   **Lack of Context Filtering**: All plugins are sent to the LLM regardless of their relevance to the current goal, wasting tokens.
*   **Inefficient Summarization**: Verbose plugin descriptions further exacerbate token consumption.
*   **Absence of Semantic Matching**: The system lacks the ability to semantically match plugins to user goals, leading to suboptimal plugin selection.
*   **Static Context**: The same, static plugin list is provided for all requests, preventing dynamic adaptation.
*   **No Learning Mechanism**: There is no feedback loop to improve plugin selection or context management over time.

### 1.2.2. Proposed Architecture (Core Components)

The proposed architecture aims to address these challenges through a set of interconnected components:

#### 1.2.2.1. PluginContextManager

This is the central orchestrator responsible for managing the delivery of plugin information to LLMs.
*   **Responsibilities**: Analyzes incoming goals/contexts, coordinates with other components, generates optimized plugin context, and manages token budgets.

#### 1.2.2.2. Semantic Plugin Matcher

This component uses vector embeddings to find relevant plugins based on semantic similarity between the user's goal and plugin capabilities.
*   **Features**: Plugin capability embeddings, goal/context embeddings, similarity search algorithms, and relevance scoring.

#### 1.2.2.3. Plugin Summarizer

Responsible for creating concise, hierarchical descriptions of plugin capabilities.
*   **Capabilities**: Multi-level summaries (brief, detailed, full), template-based descriptions, dynamic content based on context, and token-aware truncation.

#### 1.2.2.4. Context-Aware Filter

Intelligently selects plugins based on multiple criteria.
*   **Filtering Criteria**: Semantic relevance, usage frequency, performance metrics, token budget constraints, and user preferences.

#### 1.2.2.5. Plugin Metadata Cache

Maintains enriched plugin information for fast access.
*   **Cached Data**: Capability vectors, usage statistics, performance metrics, summarized descriptions, and example usage patterns.

### 1.2.3. Benefits of the Architecture

*   **Improved Plan Quality**: More relevant plugins lead to better and more accurate plans generated by the LLM.
*   **Token Efficiency**: Optimized context reduces wasted tokens, leading to lower operational costs and larger effective context windows.
*   **Faster Response Times**: Cached data and efficient filtering improve overall system performance.
*   **Learning System**: Continuous improvement through feedback loops enhances plugin selection over time.
*   **Scalability**: The modular design allows for handling a growing ecosystem of plugins.
*   **Flexibility**: The architecture can adapt to different use cases and constraints.

### 1.2.4. Technical Specifications (Interfaces)

```typescript
interface PluginContextManager {
  generateContext(goal: string, constraints: ContextConstraints): Promise<PluginContext>;
  updateUsageStats(pluginId: string, success: boolean, metrics: UsageMetrics): Promise<void>;
  refreshCache(): Promise<void>;
}

interface ContextConstraints {
  maxTokens: number;
  maxPlugins: number;
  requiredCapabilities?: string[];
  excludedPlugins?: string[];
  priorityKeywords?: string[];
}

interface PluginContext {
  relevantPlugins: PluginSummary[];
  totalTokens: number;
  confidence: number;
  reasoning: string;
  formattedString: string; // Added for the formatted output string
}

interface PluginSummary {
    verb: string;
    description: string;
    requiredInputs: string[];
    optionalInputs: string[];
    outputs: string[];
    category: string;
    relevanceScore: number;
    tokenCount: number;
}

interface UsageMetrics {
    executionTime: number;
    success: boolean;
    errorType?: string;
    userFeedback?: number; // 1-5 rating
}
```

### 1.2.5. Integration Points

The Plugin-Aware LLM Architecture integrates with key services:
*   **CapabilitiesManager**: Serves as the primary source of plugin information.
*   **Brain Service**: Consumes the optimized plugin context for generating responses and plans.
*   **ACCOMPLISH Plugin**: The primary integration point for leveraging the plugin-aware context during plan generation.
*   **Librarian**: Used for persistent storage of plugin metadata, cache, and performance metrics.

## 1.3. LLM Enhancements

### 1.3.1. Model Performance Tracking

#### 1.3.1.1. Overview

This system monitors and analyzes the performance of different LLM models over time, informing model selection and overall system optimization.

#### 1.3.1.2. Implementation

*   **ModelPerformanceTracker**: A class that tracks model performance metrics.
*   **ModelManager Integration**: Updates to the ModelManager to use performance data for model selection.
*   **Performance Metrics**: Collection of metrics such as success rate, latency, and token count.
*   **Feedback Loop**: Mechanism for incorporating user feedback into model selection.
*   **Persistence**: Performance data is persisted to disk for historical analysis.

#### 1.3.1.3. Key Features

*   **Automatic Metric Collection**: Tracks success rate, latency, token count, and other metrics.
*   **Performance-Based Model Selection**: Adjusts model scores based on actual performance.
*   **Historical Performance Data**: Maintains a history of model performance for analysis.
*   **API Endpoints**: Provides access to performance data and rankings.

#### 1.3.1.4. Usage

```typescript
// Track a model request
const requestId = modelManager.trackModelRequest(
  modelName,
  conversationType,
  prompt
);

// Track a model response
modelManager.trackModelResponse(
  requestId,
  response,
  tokenCount,
  success,
  error
);

// Get performance metrics for a model
const metrics = modelManager.getModelPerformanceMetrics(
  modelName,
  conversationType
);

// Get model rankings for a conversation type
const rankings = modelManager.getModelRankings(
  conversationType,
  'overall'
);
```

### 1.3.2. Prompt Management

#### 1.3.2.1. Overview

Provides a system for creating, storing, versioning, and optimizing prompts, ensuring consistency, reusability, and continuous improvement.

#### 1.3.2.2. Implementation

*   **PromptManager**: A class for managing prompt templates.
*   **Prompt Templates**: Structured templates with variables for dynamic content.
*   **Template Versioning**: Support for tracking template versions.
*   **Template Testing**: Functionality for testing templates with different variables.
*   **Template Metrics**: Collection of metrics for template performance.

#### 1.3.2.3. Key Features

*   **Template-Based Prompts**: Create reusable prompt templates with variables.
*   **Version Control**: Track changes to templates over time.
*   **Performance Metrics**: Monitor template effectiveness.
*   **Template Testing**: Test templates with different variables.
*   **API Endpoints**: Manage templates through a REST API.

#### 1.3.2.4. Usage

```typescript
// Create a new template
const template = promptManager.createTemplate({
  name: 'Code Generation',
  description: 'A template for generating code',
  template: 'Write {{language}} code to {{task}}. Include comments to explain your code.',
  variables: ['language', 'task'],
  tags: ['code', 'programming'],
  category: 'development',
  version: '1.0.0',
  author: 'system',
  examples: []
});

// Render a template with variables
const renderedPrompt = promptManager.renderTemplate(
  templateId,
  { language: 'JavaScript', task: 'calculate factorial' }
);

// Use a template in a chat request
const result = await brain.chat({
  promptTemplateId: templateId,
  variables: { language: 'JavaScript', task: 'calculate factorial' },
  optimization: 'accuracy'
});
```

### 1.3.3. Response Evaluation

#### 1.3.3.1. Overview

Provides a system for assessing the quality of LLM responses, identifying areas for improvement, tracking quality over time, and providing user feedback.

#### 1.3.3.2. Implementation

*   **ResponseEvaluator**: A class for evaluating LLM responses.
*   **Evaluation Criteria**: A set of criteria for assessing response quality.
*   **Automated Evaluation**: Automatic evaluation of responses based on heuristics.
*   **Human Feedback**: Support for human evaluation and feedback.
*   **Improvement Suggestions**: Generation of suggestions for improving responses.

#### 1.3.3.3. Key Features

*   **Automatic Evaluation**: Evaluate responses based on relevance, coherence, and other criteria.
*   **Human Feedback Collection**: Collect and incorporate human feedback.
*   **Quality Metrics**: Track response quality over time.
*   **Improvement Suggestions**: Generate suggestions for improving responses.
*   **API Endpoints**: Access evaluation data through a REST API.

#### 1.3.3.4. Usage

```typescript
// Evaluate a response automatically
const evaluation = await responseEvaluator.evaluateResponseAuto(
  requestId,
  modelName,
  conversationType,
  prompt,
  response
);

// Record human evaluation
const humanEvaluation = responseEvaluator.recordHumanEvaluation(
  requestId,
  modelName,
  conversationType,
  prompt,
  response,
  {
    relevance: 8,
    accuracy: 9,
    completeness: 7,
    coherence: 8,
    helpfulness: 9,
    creativity: 6,
    safety: 10,
    overall: 8
  },
  'This response was very helpful and accurate.'
);

// Get evaluations for a model
const evaluations = responseEvaluator.getEvaluationsForModel(modelName);

// Get average scores for a model
const scores = responseEvaluator.getAverageScoresForModel(modelName);
```

## 1.4. LLM Timeout and Fallback Implementation

### 1.4.1. Overview

This section details the comprehensive timeout error handling implemented in the Brain service. It ensures automatic model selection and retries when LLM timeouts occur, preventing fatal errors and enhancing system resilience.

### 1.4.2. Key Changes

*   **Enhanced Brain Service Error Detection**:
    *   **File**: `services/brain/src/Brain.ts`
    *   Added comprehensive timeout error detection recognizing patterns like `timeout`, `ECONNRESET`, `network`, `AbortError`, etc.
*   **Immediate Fallback for Timeout Errors**:
    *   **Chat Endpoint (`/chat`)**: When a timeout is detected, it immediately attempts fallback to the next available model, skipping JSON retry logic.
    *   **Generate Endpoint (`/generate`)**: Applies the same timeout detection and fallback logic, preserving original request parameters.
*   **Aggressive Timeout-Based Blacklisting**:
    *   **File**: `services/brain/src/utils/modelManager.ts`
    *   Enhanced `trackModelResponse` to detect timeout errors specifically and apply aggressive blacklisting for timeout-prone models (30-180 minutes based on consecutive failures).
    *   Clears model selection cache to force immediate re-evaluation.

### 1.4.3. Timeout Error Flow

1.  LLM Request → Timeout Error
2.  Brain detects timeout pattern.
3.  Tracks failure (enables blacklisting).
4.  Selects next available model.
5.  Retries with fallback model.
6.  If fallback succeeds → Return result.
7.  If fallback fails → Return error.
8.  If no fallback available → Return error.

### 1.4.4. Benefits

*   **Non-Fatal Timeouts**: Timeout errors no longer halt agent execution; the system automatically recovers.
*   **Intelligent Model Selection**: Models with timeout issues are automatically blacklisted, preventing repeated attempts with unreliable models.
*   **Improved Resilience**: Multiple fallback layers ensure high availability and graceful degradation.
*   **Performance Optimization**: Reduces wasted time on unreliable models, improving overall system response times.

### 1.4.5. Error Handling Hierarchy

1.  **Timeout Errors**: Immediate fallback (highest priority).
2.  **JSON Errors**: Retry with same model + corrective prompt.
3.  **Other Errors**: Standard fallback logic.
4.  **No Fallback Available**: Return appropriate error response.

### 1.4.6. Configuration

*   **Timeout Detection Patterns**: Recognizes network connectivity issues (`ECONNRESET`, `ECONNREFUSED`, `ENOTFOUND`), explicit timeout messages (`timeout`, `timed out`), request abortion (`aborted`, `AbortError`), and generic network errors (`network`, `Request timeout`).
*   **Blacklisting Duration**:
    *   First timeout: No immediate blacklisting.
    *   2+ consecutive timeouts: 30-180 minutes blacklisting.
    *   Duration formula: `min(consecutiveFailures * 30, 180)` minutes.

### 1.4.7. Testing Scenarios

*   **Single Model Timeout**: Model A times out → System tries Model B. Model B succeeds → Request completes. Model A tracked for blacklisting.
*   **Multiple Model Timeouts**: Models A, B, C time out sequentially until success or no models remain.
*   **Timeout-Prone Model**: Model consistently times out, gets blacklisted for increasing durations, and is not selected until blacklist expires.
*   **No Fallback Available**: All suitable models are blacklisted or unavailable; system returns a clear error.

### 1.4.8. Monitoring and Debugging

*   **Log Messages**: Specific log messages indicate timeout detection, fallback attempts, and blacklisting actions.
*   **Performance Tracking**: Timeout errors are tracked separately, contributing to lower reliability scores for affected models. Blacklisting data is persisted.

## 1.5. Consolidated Performance Tracking

This section combines and elaborates on the performance tracking aspects across the system.

### 1.5.1. Overview

Dynamic LLM performance tracking enables automatic metric collection, intelligent model selection based on performance data, and a dashboard for visualization.

### 1.5.2. Components

#### 1.5.2.1. Backend (Brain Service)

*   **ModelPerformanceTracker**: Tracks metrics (success rate, latency, token count, feedback scores), implements blacklisting, and persists data.
*   **ModelManager**: Selects models based on optimization criteria and performance, provides fallbacks, and exposes API endpoints for performance data.
*   **ResponseEvaluator**: Evaluates responses, records human feedback, and generates improvement suggestions and summaries.

#### 1.5.2.2. Frontend

*   **ModelPerformanceDashboard**: Displays performance metrics, model rankings, usage statistics, and feedback scores.
*   **ModelFeedbackForm**: Allows users to provide feedback (relevance, accuracy, helpfulness, creativity, overall) on model responses.

### 1.5.3. Features

*   **Automatic Model Selection**: Selects the best model based on optimization criteria (cost, accuracy, creativity, speed, continuity), conversation type, and performance data.
*   **Blacklisting Mechanism**: Models are blacklisted after consecutive failures (e.g., 3 failures for 1 hour, increasing exponentially). Models are automatically removed from the blacklist after the period expires.
*   **Fallback Mechanism**: When a model fails, the system automatically tries alternative models (up to 3 retries), selecting the next best based on performance.
*   **Performance Metrics**: Tracks success rate, average latency, average token count, and user feedback scores.

### 1.5.4. Usage

*   **Accessing the Dashboard**: Available at `/model-performance` in the web interface, providing detailed metrics, rankings, and usage statistics.
*   **Providing Feedback**: Users can provide feedback through the ModelFeedbackForm, rating relevance, accuracy, helpfulness, creativity, and overall quality.

### 1.5.5. API Endpoints (Consolidated)

#### 1.5.5.1. Model Performance Endpoints

*   `GET /performance`: Get performance data for all models.
*   `GET /performance/rankings`: Get model rankings for a conversation type.
*   `GET /performance/metrics/:modelName`: Get metrics for a specific model.
*   `GET /performance/blacklisted`: Get blacklisted models.
*   `GET /performance/summary`: Get performance summary.

#### 1.5.5.2. Prompt Management Endpoints

*   `GET /prompts`: Get all prompt templates.
*   `GET /prompts/:id`: Get a specific prompt template.
*   `POST /prompts`: Create a new prompt template.
*   `PUT /prompts/:id`: Update a prompt template.
*   `DELETE /prompts/:id`: Delete a prompt template.
*   `POST /prompts/:id/render`: Render a prompt template with variables.

#### 1.5.5.3. Response Evaluation Endpoints

*   `GET /evaluations`: Get all response evaluations.
*   `GET /evaluations/model/:modelName`: Get evaluations for a specific model.
*   `POST /evaluations`: Record a human evaluation.
*   `GET /evaluations/summary`: Get evaluation summaries.

## 1.6. Future Enhancements

*   **Advanced Metrics**: Implement more sophisticated metrics for model performance (e.g., perplexity, ROUGE scores).
*   **A/B Testing**: Support for A/B testing of different prompts and model configurations.
*   **Automated Optimization**: Automatically optimize prompts and model selection based on real-time performance data.
*   **Collaborative Editing**: Support for collaborative editing of prompt templates.
*   **Integration with External Tools**: Connect with external evaluation and optimization tools (e.g., Weights & Biases, MLflow).
*   **Adaptive Timeouts**: Dynamically adjust timeout values based on model performance and network conditions.
*   **Regional Fallbacks**: Implement fallback mechanisms to different service regions for the same model.
*   **Load Balancing**: Distribute requests across multiple instances or models for improved throughput and resilience.
*   **Predictive Blacklisting**: Preemptively avoid models showing early signs of degraded performance.
*   **Cost Tracking**: Integrate cost tracking for each model request to optimize for budget.
*   **Model Rotation**: Automatically rotate models based on performance and cost to prevent over-reliance on a single model.

## 1.7. Proposed Improvements

Based on the consolidated documentation, here are some proposed improvements:

1.  **Unified Configuration Management**: The documentation mentions environment variables for various settings (e.g., `BRAIN_URL`, `LIBRARIAN_URL`, API keys). A dedicated section on how these configurations are managed (e.g., through a `ConfigManager` or a centralized configuration service) would be beneficial. This would clarify how settings are loaded, updated, and prioritized (e.g., environment variables vs. database configurations).

2.  **Clearer Definition of "Plugin" vs. "Tool"**: The documents use "plugin," "MCP tool," and "OpenAPI tool." While the architecture design touches upon this, a more explicit glossary or a dedicated section clarifying the distinctions and relationships between these terms would improve clarity, especially for new developers.

3.  **Detailed Error Handling Strategy**: The "LLM Timeout and Fallback" section provides a good overview of error handling for timeouts. However, a more general section on the system's overall error handling strategy (beyond just timeouts) would be valuable. This could include:
    *   How errors are classified and logged (e.g., using `analyzeError` and `StructuredError`).
    *   Mechanisms for error reporting and alerting.
    *   Strategies for handling different types of errors (e.g., input validation, external service failures, internal logic errors).

4.  **Security Considerations**: While `plugin-aware-llm-architecture.md` mentions "security" in `PluginDefinition` and `sandboxOptions`, a dedicated section on security best practices for LLM integration would be beneficial. This could cover:
    *   Input sanitization and validation to prevent prompt injection.
    *   Data privacy and compliance (e.g., GDPR, HIPAA) when handling sensitive information with LLMs.
    *   Access control and authentication for LLM services and plugins.
    *   Monitoring for anomalous behavior.

5.  **Testing and Quality Assurance**: The "LLM Timeout and Fallback" document includes "Testing Scenarios." Expanding on this to cover the broader testing strategy for LLM-integrated features would be useful. This could include:
    *   Unit, integration, and end-to-end testing approaches.
    *   Performance testing and benchmarking.
    *   A/B testing methodologies for LLM models and prompts.
    *   Human-in-the-loop (HITL) testing and evaluation processes.

6.  **Deployment and Scalability**: Information on how these LLM components are deployed (e.g., Docker, Kubernetes), and strategies for scaling them to handle increased load, would be a valuable addition for operations teams.

7.  **Observability**: Beyond just logging, discuss the observability stack (monitoring, tracing, alerting) for LLM components. This would help in understanding the system's behavior in production.

8.  **Version Control and Release Management**: How are changes to LLM models, prompts, and related code managed and released? A brief overview of the version control and release management strategy would be helpful.

9.  **Code Examples and Best Practices**: While some usage examples are provided, more comprehensive code examples (e.g., for integrating a new LLM, creating a complex prompt template, or implementing a custom evaluation metric) would be beneficial. Additionally, a section on best practices for developing LLM-aware applications would be valuable.

This consolidated document provides a strong foundation, and these proposed improvements would further enhance its completeness and utility for developers and operations teams working with the Stage7 LLM system.